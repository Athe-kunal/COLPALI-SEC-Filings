{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recoverx/astarag/research/colpali/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/recoverx/astarag/research/colpali/.venv/lib/python3.10/site-packages/transformers/models/paligemma/configuration_paligemma.py:137: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.44, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.17it/s]\n",
      "Some weights of ColPali were not initialized from the model checkpoint at google/paligemma-3b-mix-448 and are newly initialized: ['custom_text_proj.bias', 'custom_text_proj.weight', 'language_model.lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "from colpali_engine.models.paligemma_colbert_architecture import ColPali\n",
    "from colpali_engine.trainer.retrieval_evaluator import CustomEvaluator\n",
    "from colpali_engine.utils.colpali_processing_utils import process_images, process_queries\n",
    "\n",
    "\n",
    "COLORS = [\"#4285f4\", \"#db4437\", \"#f4b400\", \"#0f9d58\", \"#e48ef1\"]\n",
    "# Load model\n",
    "model_name = \"vidore/colpali\"\n",
    "token = os.environ.get(\"HF_TOKEN\")\n",
    "model = ColPali.from_pretrained(\n",
    "    \"google/paligemma-3b-mix-448\", torch_dtype=torch.bfloat16, device_map=\"cuda\", token=token\n",
    ").eval()\n",
    "model.load_adapter(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name, token=token)\n",
    "device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageChops\n",
    "import numpy as np\n",
    "mock_image = Image.new(\"RGB\", (448, 448), (255, 255, 255))\n",
    "\n",
    "def search(query:str, ds,k:int=1):\n",
    "    qs = []\n",
    "    with torch.no_grad():\n",
    "        batch_query = process_queries(processor,[query],mock_image)\n",
    "        batch_query = {k: v.to(device) for k,v in batch_query.items()}\n",
    "        embeddings_query = model(**batch_query)\n",
    "        qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n",
    "    \n",
    "    retriever_evaluator = CustomEvaluator(is_multi_vector=True)\n",
    "    scores = retriever_evaluator.evaluate(qs,ds)\n",
    "    best_pages_idxs = np.argsort(scores,axis=1).squeeze(0)\n",
    "    return best_pages_idxs[::-1][:k]\n",
    "\n",
    "\n",
    "def trim_and_square(im, padding=10, target_size=448):\n",
    "    # Trim whitespace\n",
    "    # print(im)\n",
    "    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))\n",
    "    diff = ImageChops.difference(im, bg)\n",
    "    diff = ImageChops.add(diff, diff, 2.0, -100)\n",
    "    bbox = diff.getbbox()\n",
    "    if bbox:\n",
    "        # Add padding to the bounding box\n",
    "        left, top, right, bottom = bbox\n",
    "        left = max(0, left - padding)\n",
    "        top = max(0, top - padding)\n",
    "        right = min(im.width, right + padding)\n",
    "        bottom = min(im.height, bottom + padding)\n",
    "        trimmed = im.crop((left, top, right, bottom))\n",
    "    else:\n",
    "        trimmed = im\n",
    "\n",
    "    # Resize to square\n",
    "    w, h = trimmed.size\n",
    "    if w > h:\n",
    "        new_w = target_size\n",
    "        new_h = int(h * (target_size / w))\n",
    "    else:\n",
    "        new_h = target_size\n",
    "        new_w = int(w * (target_size / h))\n",
    "    \n",
    "    resized = trimmed.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "    # Create a white square image\n",
    "    square = Image.new('RGB', (target_size, target_size), (255, 255, 255))\n",
    "    \n",
    "    # Paste the resized image onto the square, centered\n",
    "    paste_x = (target_size - new_w) // 2\n",
    "    paste_y = (target_size - new_h) // 2\n",
    "    square.paste(resized, (paste_x, paste_y))\n",
    "\n",
    "    return square\n",
    "\n",
    "def index(file,ds):\n",
    "    images = []\n",
    "    for f in file:\n",
    "        print(f)\n",
    "        f_imgs = convert_from_path(f)\n",
    "        for f_img in f_imgs:\n",
    "            cropped_f_img = trim_and_square(f_img)\n",
    "            images.append(cropped_f_img)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        images,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: process_images(processor,x)\n",
    "    )\n",
    "    \n",
    "    for batch_doc in tqdm(dataloader):\n",
    "        with torch.no_grad():\n",
    "            batch_doc = {k: v.to(device) for k,v in batch_doc.items()}\n",
    "            embeddings_doc = model(**batch_doc)\n",
    "        ds.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))\n",
    "    return f\"Uploaded and converted {len(images)} pages\", ds, images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/SEC_EDGAR_FILINGS/GOOG-2024/goog-20240630-10-Q2.pdf\n",
      "output/SEC_EDGAR_FILINGS/GOOG-2024/goog-20240331-10-Q1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:06<00:00,  3.63it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = []\n",
    "files = [os.path.join(\"output/SEC_EDGAR_FILINGS/GOOG-2024\",f) for f in os.listdir(\"output/SEC_EDGAR_FILINGS/GOOG-2024\") if f.endswith(\".pdf\")]\n",
    "msg, ds, images = index(files,ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([84])\n",
      "Top 1 Accuracy (verif): 0.0\n"
     ]
    }
   ],
   "source": [
    "# query = \"What was the operating expense of Google for the year?\"\n",
    "query = \"What is the total revenue generated?\"\n",
    "\n",
    "best_img_idxs = search(query,ds,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([84, 86, 11, 36, 31])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_img_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from dataclasses import asdict, dataclass\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from PIL import Image\n",
    "from tqdm import trange\n",
    "\n",
    "from colpali_engine.interpretability.plot_utils import plot_patches\n",
    "from colpali_engine.interpretability.processor import ColPaliProcessor\n",
    "from colpali_engine.interpretability.torch_utils import normalize_attention_map_per_query_token\n",
    "from colpali_engine.interpretability.vit_configs import VIT_CONFIG\n",
    "from colpali_engine.models.paligemma_colbert_architecture import ColPali\n",
    "\n",
    "OUTDIR_INTERPRETABILITY = Path(\"outputs/interpretability\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InterpretabilityInput:\n",
    "    query: str\n",
    "    image: Image.Image\n",
    "    start_idx_token: int\n",
    "    end_idx_token: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interpretability_plots(\n",
    "    model: ColPali,\n",
    "    processor: ColPaliProcessor,\n",
    "    query: str,\n",
    "    image: Image.Image,\n",
    "    add_special_prompt_to_doc: bool = True,\n",
    ") -> None:\n",
    "\n",
    "    # Sanity checks\n",
    "    if len(model.active_adapters()) != 1:\n",
    "        raise ValueError(\"The model must have exactly one active adapter.\")\n",
    "\n",
    "    if model.config.name_or_path not in VIT_CONFIG:\n",
    "        raise ValueError(\"The model must be referred to in the VIT_CONFIG dictionary.\")\n",
    "    vit_config = VIT_CONFIG[model.config.name_or_path]\n",
    "    # Preprocess the inputs\n",
    "    input_text_processed = processor.process_text(query).to(model.device)\n",
    "    input_image_processed = processor.process_image(image, add_special_prompt=add_special_prompt_to_doc).to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output_text = model.forward(**asdict(input_text_processed))  # (1, n_text_tokens, hidden_dim)\n",
    "\n",
    "    # NOTE: `output_image`` will have shape:\n",
    "    # (1, n_patch_x * n_patch_y, hidden_dim) if `add_special_prompt_to_doc` is False\n",
    "    # (1, n_patch_x * n_patch_y + n_special_tokens, hidden_dim) if `add_special_prompt_to_doc` is True\n",
    "    with torch.no_grad():\n",
    "        output_image = model.forward(**asdict(input_image_processed))\n",
    "\n",
    "    if add_special_prompt_to_doc:  # remove the special tokens\n",
    "        output_image = output_image[\n",
    "            :, : processor.processor.image_seq_length, :\n",
    "        ]  # (1, n_patch_x * n_patch_y, hidden_dim)\n",
    "\n",
    "    output_image = rearrange(\n",
    "        output_image, \"b (h w) c -> b h w c\", h=vit_config.n_patch_per_dim, w=vit_config.n_patch_per_dim\n",
    "    )  # (1, n_patch_x, n_patch_y, hidden_dim)\n",
    "\n",
    "    # Get the unnormalized attention map\n",
    "    attention_map = torch.einsum(\n",
    "        \"bnk,bijk->bnij\", output_text, output_image\n",
    "    )  # (1, n_text_tokens, n_patch_x, n_patch_y)\n",
    "    attention_map_normalized = normalize_attention_map_per_query_token(\n",
    "        attention_map\n",
    "    )  # (1, n_text_tokens, n_patch_x, n_patch_y)\n",
    "    attention_map_normalized = attention_map_normalized.float()\n",
    "\n",
    "    # Get text token information\n",
    "    text_tokens = processor.tokenizer.tokenize(processor.decode(input_text_processed.input_ids[0]))\n",
    "    # print(\"Text tokens:\")\n",
    "    # pprint.pprint(text_tokens)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    return attention_map_normalized, attention_map,text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_map_normalized, attention_map,text_tokens = generate_interpretability_plots(\n",
    "    model,\n",
    "    ColPaliProcessor(processor=processor),\n",
    "    query,\n",
    "    images[34],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
